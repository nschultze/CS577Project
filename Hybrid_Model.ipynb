{"cells":[{"cell_type":"markdown","metadata":{"id":"Z7_hx8SJJR4-"},"source":["# Hybrid Model (CNN and Transformer) - Convolutional Vision Transformer\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8110,"status":"ok","timestamp":1701142387034,"user":{"displayName":"Nathan Loh","userId":"06388712244962875825"},"user_tz":360},"id":"FlgiGLhybn4V"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, utils, Sequential\n","from tensorflow.keras.layers import MultiHeadAttention\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, accuracy_score, f1_score\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"HZ0Qv6lnbn4X"},"source":["### Reading The Data"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":589137,"status":"ok","timestamp":1701142976167,"user":{"displayName":"Nathan Loh","userId":"06388712244962875825"},"user_tz":360},"id":"Spoi9nXSddHn","outputId":"9fe9432f-5860-43f6-c27c-fea7531a3715"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CS577Project'...\n","remote: Enumerating objects: 2949, done.\u001b[K\n","remote: Counting objects: 100% (1153/1153), done.\u001b[K\n","remote: Compressing objects: 100% (1098/1098), done.\u001b[K\n","remote: Total 2949 (delta 75), reused 1125 (delta 54), pack-reused 1796\u001b[K\n","Receiving objects: 100% (2949/2949), 10.07 GiB | 37.91 MiB/s, done.\n","Resolving deltas: 100% (120/120), done.\n","Updating files: 100% (2720/2720), done.\n"]}],"source":["!git clone https://github.com/nschultze/CS577Project.git"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1701142976492,"user":{"displayName":"Nathan Loh","userId":"06388712244962875825"},"user_tz":360},"id":"ZlnsF9m_bn4X","outputId":"0984ca13-e853-472c-e36b-dff4777058db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1887 images belonging to 2 classes.\n","Found 402 images belonging to 2 classes.\n","Found 410 images belonging to 2 classes.\n"]}],"source":["train_data_dir = 'CS577Project/train'\n","validation_data_dir = 'CS577Project/val'\n","test_data_dir = 'CS577Project/test'\n","\n","#set the size of the image to be resized\n","target_size = (224, 224)\n","\n","#using ImageDataGenerator for data augmentation and preprocessing (altering images to improve data variety)\n","datagen = ImageDataGenerator(\n","    rescale=1./255, #used to normalize the pixel values (dividing by max pixel value of 255)\n","    rotation_range=20, #randomly rotating the images by 20 degrees (almost like noise where we add variation)\n","    width_shift_range=0.2, #randomly shifting the images horizontally by 20 percent\n","    height_shift_range=0.2, #randomly shifting the images vertically by 20 percent\n","    shear_range=0.2, #setting the shear intensity to \"stretch\" the image\n","    zoom_range=0.2, #randomly zooming into the images by 20 percent\n","    horizontal_flip=True, #randomly flipping the images horizontally\n","    fill_mode='nearest') #because of the shifting, rotating, etc, there are missing pixels so we fill in those missing pixels with nearest valid values\n","\n","#creating the generator for the training dataset\n","train_generator = datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=target_size, #resizing\n","    batch_size=32,\n","    class_mode='binary') #binary for 2 labels\n","\n","val_test_gen = ImageDataGenerator(rescale=1.0 / 255.0)\n","\n","#creating the generator for the validation set\n","validation_generator = val_test_gen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=target_size,\n","    batch_size=32,\n","    class_mode='binary',\n","    shuffle=False)\n","\n","#creating the generator for the testing dataset\n","test_generator = val_test_gen.flow_from_directory(\n","    test_data_dir,\n","    target_size=target_size,\n","    batch_size=32,\n","    class_mode='binary',\n","    shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rag7OcFKCVI","outputId":"477a0c27-b73d-40c6-e024-8fc88310e172"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_10 (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," efficientnetb0 (Functional  (None, 8, 8, 1280)           4049571   ['input_10[0][0]']            \n"," )                                                                                                \n","                                                                                                  \n"," reshape_5 (Reshape)         (None, 49, 1280)             0         ['efficientnetb0[0][0]']      \n","                                                                                                  \n"," reshape_6 (Reshape)         (None, 1, 49, 1280)          0         ['reshape_5[0][0]']           \n","                                                                                                  \n"," multi_head_attention_2 (Mu  (None, 1, 49, 1280)          6558720   ['reshape_6[0][0]',           \n"," ltiHeadAttention)                                                   'reshape_6[0][0]']           \n","                                                                                                  \n"," global_average_pooling2d (  (None, 1280)                 0         ['multi_head_attention_2[0][0]\n"," GlobalAveragePooling2D)                                            ']                            \n","                                                                                                  \n"," dense (Dense)               (None, 256)                  327936    ['global_average_pooling2d[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," dropout (Dropout)           (None, 256)                  0         ['dense[0][0]']               \n","                                                                                                  \n"," batch_normalization (Batch  (None, 256)                  1024      ['dropout[0][0]']             \n"," Normalization)                                                                                   \n","                                                                                                  \n"," dense_1 (Dense)             (None, 1)                    257       ['batch_normalization[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 10937508 (41.72 MB)\n","Trainable params: 10894973 (41.56 MB)\n","Non-trainable params: 42535 (166.16 KB)\n","__________________________________________________________________________________________________\n","Epoch 1/25\n","11/59 [====>.........................] - ETA: 12:58 - loss: 0.7058 - accuracy: 0.6761"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3167: DecompressionBombWarning: Image size (104688771 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["44/59 [=====================>........] - ETA: 3:45 - loss: 0.4803 - accuracy: 0.7932"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:3167: DecompressionBombWarning: Image size (89747104 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["59/59 [==============================] - 1088s 18s/step - loss: 0.4485 - accuracy: 0.8060 - val_loss: 0.8014 - val_accuracy: 0.6119 - lr: 0.0010\n","Epoch 2/25\n","59/59 [==============================] - 1028s 17s/step - loss: 0.3102 - accuracy: 0.8691 - val_loss: 1.1421 - val_accuracy: 0.6119 - lr: 0.0010\n","Epoch 3/25\n","59/59 [==============================] - 1018s 17s/step - loss: 0.2870 - accuracy: 0.8802 - val_loss: 0.8258 - val_accuracy: 0.6119 - lr: 0.0010\n","Epoch 4/25\n","59/59 [==============================] - 1045s 18s/step - loss: 0.2367 - accuracy: 0.9083 - val_loss: 0.8229 - val_accuracy: 0.6119 - lr: 0.0010\n","Epoch 5/25\n","59/59 [==============================] - 1043s 18s/step - loss: 0.1800 - accuracy: 0.9332 - val_loss: 0.6728 - val_accuracy: 0.6144 - lr: 2.0000e-04\n","Epoch 6/25\n","59/59 [==============================] - 1031s 17s/step - loss: 0.1273 - accuracy: 0.9528 - val_loss: 0.7907 - val_accuracy: 0.6219 - lr: 2.0000e-04\n","Epoch 7/25\n","59/59 [==============================] - 1027s 17s/step - loss: 0.1160 - accuracy: 0.9523 - val_loss: 0.7840 - val_accuracy: 0.6244 - lr: 2.0000e-04\n","Epoch 8/25\n","59/59 [==============================] - 1053s 18s/step - loss: 0.1165 - accuracy: 0.9565 - val_loss: 1.2284 - val_accuracy: 0.6169 - lr: 2.0000e-04\n","Epoch 9/25\n","59/59 [==============================] - 1037s 17s/step - loss: 0.0909 - accuracy: 0.9650 - val_loss: 1.1035 - val_accuracy: 0.6169 - lr: 4.0000e-05\n","Epoch 10/25\n","59/59 [==============================] - 1032s 17s/step - loss: 0.0888 - accuracy: 0.9661 - val_loss: 1.1016 - val_accuracy: 0.6766 - lr: 4.0000e-05\n","Epoch 11/25\n","59/59 [==============================] - 1029s 17s/step - loss: 0.0766 - accuracy: 0.9724 - val_loss: 0.5008 - val_accuracy: 0.8234 - lr: 4.0000e-05\n","Epoch 12/25\n","59/59 [==============================] - 1002s 17s/step - loss: 0.0745 - accuracy: 0.9724 - val_loss: 0.7522 - val_accuracy: 0.7239 - lr: 4.0000e-05\n","Epoch 13/25\n","59/59 [==============================] - 1018s 17s/step - loss: 0.0932 - accuracy: 0.9677 - val_loss: 0.5470 - val_accuracy: 0.8259 - lr: 4.0000e-05\n","Epoch 14/25\n","59/59 [==============================] - 992s 17s/step - loss: 0.0722 - accuracy: 0.9730 - val_loss: 0.5234 - val_accuracy: 0.8159 - lr: 4.0000e-05\n","Epoch 15/25\n","59/59 [==============================] - 1013s 17s/step - loss: 0.0894 - accuracy: 0.9677 - val_loss: 0.4013 - val_accuracy: 0.8582 - lr: 8.0000e-06\n","Epoch 16/25\n","59/59 [==============================] - 1007s 17s/step - loss: 0.0773 - accuracy: 0.9682 - val_loss: 0.3103 - val_accuracy: 0.8881 - lr: 8.0000e-06\n","Epoch 17/25\n","59/59 [==============================] - 1018s 17s/step - loss: 0.0673 - accuracy: 0.9783 - val_loss: 0.3113 - val_accuracy: 0.8955 - lr: 8.0000e-06\n","Epoch 18/25\n","59/59 [==============================] - 1019s 17s/step - loss: 0.0838 - accuracy: 0.9671 - val_loss: 0.3338 - val_accuracy: 0.8831 - lr: 8.0000e-06\n","Epoch 19/25\n","59/59 [==============================] - 999s 17s/step - loss: 0.0675 - accuracy: 0.9730 - val_loss: 0.3363 - val_accuracy: 0.8806 - lr: 8.0000e-06\n","Epoch 20/25\n","59/59 [==============================] - 1003s 17s/step - loss: 0.0847 - accuracy: 0.9682 - val_loss: 0.2714 - val_accuracy: 0.9055 - lr: 1.6000e-06\n","Epoch 21/25\n","59/59 [==============================] - 1019s 17s/step - loss: 0.0797 - accuracy: 0.9687 - val_loss: 0.2572 - val_accuracy: 0.9154 - lr: 1.6000e-06\n","Epoch 22/25\n","14/59 [======>.......................] - ETA: 11:16 - loss: 0.0992 - accuracy: 0.9665"]}],"source":["#Replicating CvT (Convolutional Vision Transformer)\n","#building the EfficientNetB0 model section (CNN section)\n","efficient_net = EfficientNetB0(include_top=False, input_shape=(256, 256, 3), weights='imagenet') #do not freeze the weights (want to train the weights more like transfer learning)\n","\n","#input value with the target/altered size\n","inp = tf.keras.Input(shape=(224, 224, 3))\n","\n","#building the CvT model\n","x = efficient_net(inp)\n","x = layers.Reshape((-1, 1280))(x) #resize to accommodate the output of efficient net\n","\n","patch_size = 32\n","num_patches = (224 // patch_size) * (224 // patch_size)\n","x = layers.Reshape((-1, num_patches, x.shape[-1]))(x)\n","x = MultiHeadAttention(num_heads=4, key_dim=x.shape[-1] // 4, dropout=0.1)(x, x)\n","#x = MultiHeadAttention(num_heads=4, key_dim=1280//4, dropout=0.1)(x, x) #adding the multi head attention to allow it to focus on different parts of the input\n","x = layers.GlobalAveragePooling2D()(x) #pooling to reduce input\n","x = layers.Dense(256, activation='relu')(x)\n","x = layers.Dropout(0.2)(x)\n","#adding a batch normalization for regularization\n","x = layers.BatchNormalization()(x)\n","#final output is passed into single neuron using sigmoid (good for binary classification)\n","output = layers.Dense(1, activation='sigmoid')(x)\n","\n","model = tf.keras.Model(inputs=inp, outputs=output)\n","\n","#compiling the model with an initial lr of 1e-3 (will change as training)\n","model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n","#used to change the learning rate as it is training\n","# reduces learning rate based on validation loss\n","lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n","\n","#printing the model\n","model.summary()\n","\n","#training the model\n","history = model.fit(train_generator,\n","                    epochs=40,\n","                    validation_data=validation_generator,\n","                    callbacks=[lr_scheduler])  #adding the learning rate scheduler to change learning rate as it trains\n","\n","\n","#saving the model as a HDF5 file\n","model.save(\"cvt_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kurd5Jn-Wm6s"},"outputs":[],"source":["#plotting the two figures of loss and accuracy\n","plt.figure(figsize=(12, 4))\n","\n","#plotting the training & validation losses\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Training vs. Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(['Training', 'Validation'], loc='upper right') #placed at top right as loss should decrease as epochs increase\n","\n","#plotting the training & validation accuracy\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Training vs. Validation Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(['Training', 'Validation'], loc='lower right') #moved to lower right to not obstruct lines (acc should go upwards as epochs increase)\n","\n","#saving the plots\n","plt.savefig(\"training_model_graphs.png\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"En44bDyjUl0J"},"outputs":[],"source":["#evaluating the model on the testing set\n","test_loss, test_acc = model.evaluate(test_generator)\n","print(f'\\nTest accuracy: {test_acc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sYtoI-Wbn4b"},"outputs":[],"source":["#loading the model\n","loaded_model = tf.keras.models.load_model(\"cvt_model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Ax9mRPsh7Ap"},"outputs":[],"source":["# Generate predictions from the test set\n","pred = loaded_model.predict(test_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CS4fpm3piMEB"},"outputs":[],"source":["threshold = 0.5  # Set your chosen threshold here\n","\n","# Manual computation of binary predictions\n","pred_binary = (pred >= .5).astype(int).flatten()\n","\n","# True labels\n","true_y = test_generator.classes\n","\n","# Compute confusion matrix\n","confusion_mat = confusion_matrix(true_y, pred_binary)\n","\n","# Display the confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=['Fire', 'No Fire'], yticklabels=['Fire', 'No Fire'])\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix of the Hybrid Model')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ce5ueycclNPc"},"outputs":[],"source":["print(classification_report(true_y, pred_binary, target_names = [\"Fire\", \"No Fire\"]))\n","\n","precision = precision_score(true_y, pred_binary, average='macro')\n","recall = recall_score(true_y, pred_binary, average='macro')\n","accuracy = accuracy_score(true_y, pred_binary)\n","f1 = f1_score(true_y, pred_binary, average='macro')\n","\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n","print(f'Accuracy: {accuracy}')\n","print(f'F1 Score: {f1}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"InaRWd4mAOto","executionInfo":{"status":"aborted","timestamp":1701142981524,"user_tz":360,"elapsed":601733,"user":{"displayName":"Nathan Loh","userId":"06388712244962875825"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"2d600abc6e7e43f794039ae8c357f343","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"nbformat":4,"nbformat_minor":0}